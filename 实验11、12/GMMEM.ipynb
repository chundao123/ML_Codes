{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 实验十二：EM算法和高斯混合聚类"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "要求：完成Estep和Mstep函数"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-05T06:56:32.111446Z",
     "start_time": "2025-01-05T06:56:32.108208Z"
    }
   },
   "source": [
    "import os\n",
    "os.environ['OMP_NUM_THREADS'] = '6'\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 基于sklearn的高斯混合模型对人脸图像数据集进行聚类分析"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "数据离线使用方式：将数据集下载到本地，并解压到工作目录\n",
    "```\n",
    "cwd\n",
    "├─lfw_home\n",
    "│  ├─joblib\n",
    "│  ├─lfw_funneled\n",
    "│  ├─pairs.txt\n",
    "│  ├─pairsDevTest.txt\n",
    "│  └─pairsDevTrain.txt\n",
    "├─GMM&EM.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-05T06:56:37.485788Z",
     "start_time": "2025-01-05T06:56:32.120520Z"
    }
   },
   "source": [
    "# 下载LFW人脸数据集\n",
    "from sklearn.datasets import fetch_lfw_people\n",
    "lfw_people = fetch_lfw_people(data_home=os.getcwd(),min_faces_per_person=70, resize=0.4)\n",
    "X = lfw_people.data  # 提取图像数据\n",
    "n_samples, h, w = lfw_people.images.shape  # 获取图像的尺寸"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Env\\FuckPytorchFuckCuda\\Lib\\site-packages\\sklearn\\datasets\\_base.py:1472: UserWarning: Retry downloading from url: https://ndownloader.figshare.com/files/5976012\n",
      "  warnings.warn(f\"Retry downloading from url: {remote.url}\")\n"
     ]
    },
    {
     "ename": "HTTPError",
     "evalue": "HTTP Error 403: Forbidden",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mHTTPError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[4], line 3\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;66;03m# 下载LFW人脸数据集\u001B[39;00m\n\u001B[0;32m      2\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01msklearn\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdatasets\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m fetch_lfw_people\n\u001B[1;32m----> 3\u001B[0m lfw_people \u001B[38;5;241m=\u001B[39m \u001B[43mfetch_lfw_people\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdata_home\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mos\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgetcwd\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43mmin_faces_per_person\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m70\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mresize\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m0.4\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m      4\u001B[0m X \u001B[38;5;241m=\u001B[39m lfw_people\u001B[38;5;241m.\u001B[39mdata  \u001B[38;5;66;03m# 提取图像数据\u001B[39;00m\n\u001B[0;32m      5\u001B[0m n_samples, h, w \u001B[38;5;241m=\u001B[39m lfw_people\u001B[38;5;241m.\u001B[39mimages\u001B[38;5;241m.\u001B[39mshape  \u001B[38;5;66;03m# 获取图像的尺寸\u001B[39;00m\n",
      "File \u001B[1;32mD:\\Env\\FuckPytorchFuckCuda\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:213\u001B[0m, in \u001B[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    207\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m    208\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m config_context(\n\u001B[0;32m    209\u001B[0m         skip_parameter_validation\u001B[38;5;241m=\u001B[39m(\n\u001B[0;32m    210\u001B[0m             prefer_skip_nested_validation \u001B[38;5;129;01mor\u001B[39;00m global_skip_validation\n\u001B[0;32m    211\u001B[0m         )\n\u001B[0;32m    212\u001B[0m     ):\n\u001B[1;32m--> 213\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    214\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m InvalidParameterError \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m    215\u001B[0m     \u001B[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001B[39;00m\n\u001B[0;32m    216\u001B[0m     \u001B[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001B[39;00m\n\u001B[0;32m    217\u001B[0m     \u001B[38;5;66;03m# the name of the estimator by the name of the function in the error\u001B[39;00m\n\u001B[0;32m    218\u001B[0m     \u001B[38;5;66;03m# message to avoid confusion.\u001B[39;00m\n\u001B[0;32m    219\u001B[0m     msg \u001B[38;5;241m=\u001B[39m re\u001B[38;5;241m.\u001B[39msub(\n\u001B[0;32m    220\u001B[0m         \u001B[38;5;124mr\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mparameter of \u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124mw+ must be\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m    221\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mparameter of \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mfunc\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__qualname__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m must be\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m    222\u001B[0m         \u001B[38;5;28mstr\u001B[39m(e),\n\u001B[0;32m    223\u001B[0m     )\n",
      "File \u001B[1;32mD:\\Env\\FuckPytorchFuckCuda\\Lib\\site-packages\\sklearn\\datasets\\_lfw.py:384\u001B[0m, in \u001B[0;36mfetch_lfw_people\u001B[1;34m(data_home, funneled, resize, min_faces_per_person, color, slice_, download_if_missing, return_X_y, n_retries, delay)\u001B[0m\n\u001B[0;32m    244\u001B[0m \u001B[38;5;129m@validate_params\u001B[39m(\n\u001B[0;32m    245\u001B[0m     {\n\u001B[0;32m    246\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdata_home\u001B[39m\u001B[38;5;124m\"\u001B[39m: [\u001B[38;5;28mstr\u001B[39m, PathLike, \u001B[38;5;28;01mNone\u001B[39;00m],\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    270\u001B[0m     delay\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1.0\u001B[39m,\n\u001B[0;32m    271\u001B[0m ):\n\u001B[0;32m    272\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Load the Labeled Faces in the Wild (LFW) people dataset \\\u001B[39;00m\n\u001B[0;32m    273\u001B[0m \u001B[38;5;124;03m(classification).\u001B[39;00m\n\u001B[0;32m    274\u001B[0m \n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    382\u001B[0m \u001B[38;5;124;03m    Aaron Patterson\u001B[39;00m\n\u001B[0;32m    383\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m--> 384\u001B[0m     lfw_home, data_folder_path \u001B[38;5;241m=\u001B[39m \u001B[43m_check_fetch_lfw\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    385\u001B[0m \u001B[43m        \u001B[49m\u001B[43mdata_home\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdata_home\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    386\u001B[0m \u001B[43m        \u001B[49m\u001B[43mfunneled\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mfunneled\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    387\u001B[0m \u001B[43m        \u001B[49m\u001B[43mdownload_if_missing\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdownload_if_missing\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    388\u001B[0m \u001B[43m        \u001B[49m\u001B[43mn_retries\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mn_retries\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    389\u001B[0m \u001B[43m        \u001B[49m\u001B[43mdelay\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdelay\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    390\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    391\u001B[0m     logger\u001B[38;5;241m.\u001B[39mdebug(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mLoading LFW people faces from \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m\"\u001B[39m, lfw_home)\n\u001B[0;32m    393\u001B[0m     \u001B[38;5;66;03m# wrap the loader in a memoizing function that will return memmaped data\u001B[39;00m\n\u001B[0;32m    394\u001B[0m     \u001B[38;5;66;03m# arrays for optimal memory usage\u001B[39;00m\n",
      "File \u001B[1;32mD:\\Env\\FuckPytorchFuckCuda\\Lib\\site-packages\\sklearn\\datasets\\_lfw.py:93\u001B[0m, in \u001B[0;36m_check_fetch_lfw\u001B[1;34m(data_home, funneled, download_if_missing, n_retries, delay)\u001B[0m\n\u001B[0;32m     91\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m download_if_missing:\n\u001B[0;32m     92\u001B[0m     logger\u001B[38;5;241m.\u001B[39minfo(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDownloading LFW metadata: \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m\"\u001B[39m, target\u001B[38;5;241m.\u001B[39murl)\n\u001B[1;32m---> 93\u001B[0m     \u001B[43m_fetch_remote\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m     94\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtarget\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdirname\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mlfw_home\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mn_retries\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mn_retries\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdelay\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdelay\u001B[49m\n\u001B[0;32m     95\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     96\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m     97\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mOSError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m is missing\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m%\u001B[39m target_filepath)\n",
      "File \u001B[1;32mD:\\Env\\FuckPytorchFuckCuda\\Lib\\site-packages\\sklearn\\datasets\\_base.py:1466\u001B[0m, in \u001B[0;36m_fetch_remote\u001B[1;34m(remote, dirname, n_retries, delay)\u001B[0m\n\u001B[0;32m   1464\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[0;32m   1465\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m-> 1466\u001B[0m         \u001B[43murlretrieve\u001B[49m\u001B[43m(\u001B[49m\u001B[43mremote\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43murl\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfile_path\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1467\u001B[0m         \u001B[38;5;28;01mbreak\u001B[39;00m\n\u001B[0;32m   1468\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m (URLError, \u001B[38;5;167;01mTimeoutError\u001B[39;00m):\n",
      "File \u001B[1;32mD:\\Python312\\Lib\\urllib\\request.py:240\u001B[0m, in \u001B[0;36murlretrieve\u001B[1;34m(url, filename, reporthook, data)\u001B[0m\n\u001B[0;32m    223\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    224\u001B[0m \u001B[38;5;124;03mRetrieve a URL into a temporary location on disk.\u001B[39;00m\n\u001B[0;32m    225\u001B[0m \n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    236\u001B[0m \u001B[38;5;124;03mdata file as well as the resulting HTTPMessage object.\u001B[39;00m\n\u001B[0;32m    237\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    238\u001B[0m url_type, path \u001B[38;5;241m=\u001B[39m _splittype(url)\n\u001B[1;32m--> 240\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m contextlib\u001B[38;5;241m.\u001B[39mclosing(\u001B[43murlopen\u001B[49m\u001B[43m(\u001B[49m\u001B[43murl\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdata\u001B[49m\u001B[43m)\u001B[49m) \u001B[38;5;28;01mas\u001B[39;00m fp:\n\u001B[0;32m    241\u001B[0m     headers \u001B[38;5;241m=\u001B[39m fp\u001B[38;5;241m.\u001B[39minfo()\n\u001B[0;32m    243\u001B[0m     \u001B[38;5;66;03m# Just return the local path and the \"headers\" for file://\u001B[39;00m\n\u001B[0;32m    244\u001B[0m     \u001B[38;5;66;03m# URLs. No sense in performing a copy unless requested.\u001B[39;00m\n",
      "File \u001B[1;32mD:\\Python312\\Lib\\urllib\\request.py:215\u001B[0m, in \u001B[0;36murlopen\u001B[1;34m(url, data, timeout, cafile, capath, cadefault, context)\u001B[0m\n\u001B[0;32m    213\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    214\u001B[0m     opener \u001B[38;5;241m=\u001B[39m _opener\n\u001B[1;32m--> 215\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mopener\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mopen\u001B[49m\u001B[43m(\u001B[49m\u001B[43murl\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdata\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtimeout\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mD:\\Python312\\Lib\\urllib\\request.py:521\u001B[0m, in \u001B[0;36mOpenerDirector.open\u001B[1;34m(self, fullurl, data, timeout)\u001B[0m\n\u001B[0;32m    519\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m processor \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mprocess_response\u001B[38;5;241m.\u001B[39mget(protocol, []):\n\u001B[0;32m    520\u001B[0m     meth \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mgetattr\u001B[39m(processor, meth_name)\n\u001B[1;32m--> 521\u001B[0m     response \u001B[38;5;241m=\u001B[39m \u001B[43mmeth\u001B[49m\u001B[43m(\u001B[49m\u001B[43mreq\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mresponse\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    523\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m response\n",
      "File \u001B[1;32mD:\\Python312\\Lib\\urllib\\request.py:630\u001B[0m, in \u001B[0;36mHTTPErrorProcessor.http_response\u001B[1;34m(self, request, response)\u001B[0m\n\u001B[0;32m    627\u001B[0m \u001B[38;5;66;03m# According to RFC 2616, \"2xx\" code indicates that the client's\u001B[39;00m\n\u001B[0;32m    628\u001B[0m \u001B[38;5;66;03m# request was successfully received, understood, and accepted.\u001B[39;00m\n\u001B[0;32m    629\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;241m200\u001B[39m \u001B[38;5;241m<\u001B[39m\u001B[38;5;241m=\u001B[39m code \u001B[38;5;241m<\u001B[39m \u001B[38;5;241m300\u001B[39m):\n\u001B[1;32m--> 630\u001B[0m     response \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mparent\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43merror\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    631\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mhttp\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrequest\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mresponse\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcode\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmsg\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mhdrs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    633\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m response\n",
      "File \u001B[1;32mD:\\Python312\\Lib\\urllib\\request.py:559\u001B[0m, in \u001B[0;36mOpenerDirector.error\u001B[1;34m(self, proto, *args)\u001B[0m\n\u001B[0;32m    557\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m http_err:\n\u001B[0;32m    558\u001B[0m     args \u001B[38;5;241m=\u001B[39m (\u001B[38;5;28mdict\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdefault\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mhttp_error_default\u001B[39m\u001B[38;5;124m'\u001B[39m) \u001B[38;5;241m+\u001B[39m orig_args\n\u001B[1;32m--> 559\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_chain\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mD:\\Python312\\Lib\\urllib\\request.py:492\u001B[0m, in \u001B[0;36mOpenerDirector._call_chain\u001B[1;34m(self, chain, kind, meth_name, *args)\u001B[0m\n\u001B[0;32m    490\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m handler \u001B[38;5;129;01min\u001B[39;00m handlers:\n\u001B[0;32m    491\u001B[0m     func \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mgetattr\u001B[39m(handler, meth_name)\n\u001B[1;32m--> 492\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    493\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m result \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    494\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m result\n",
      "File \u001B[1;32mD:\\Python312\\Lib\\urllib\\request.py:639\u001B[0m, in \u001B[0;36mHTTPDefaultErrorHandler.http_error_default\u001B[1;34m(self, req, fp, code, msg, hdrs)\u001B[0m\n\u001B[0;32m    638\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mhttp_error_default\u001B[39m(\u001B[38;5;28mself\u001B[39m, req, fp, code, msg, hdrs):\n\u001B[1;32m--> 639\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m HTTPError(req\u001B[38;5;241m.\u001B[39mfull_url, code, msg, hdrs, fp)\n",
      "\u001B[1;31mHTTPError\u001B[0m: HTTP Error 403: Forbidden"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# 输出数据形状\n",
    "print(\"Number of samples:\", n_samples)\n",
    "print(\"Image height:\", h)\n",
    "print(\"Image width:\", w)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# 数据标准化处理\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_normalized = scaler.fit_transform(X)\n",
    "\n",
    "# 输出标准化后的数据形状\n",
    "print(\"Normalized data shape:\", X_normalized.shape)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# 使用PCA进行降维\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "n_components = 150  # 降维到150维\n",
    "pca = PCA(n_components=n_components, whiten=True).fit(X_normalized)\n",
    "X_pca = pca.transform(X_normalized)\n",
    "\n",
    "# 输出降维后的数据形状\n",
    "print(\"PCA transformed data shape:\", X_pca.shape)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# 高斯混合模型初始化和训练\n",
    "\n",
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "\n",
    "n_clusters = 8  # 假设我们要将面部图像分成8类\n",
    "gmm = GaussianMixture(n_components=n_clusters, covariance_type='full', random_state=42)\n",
    "gmm.fit(X_pca)\n",
    "\n",
    "# 输出模型参数\n",
    "print(\"Means shape:\", gmm.means_.shape)\n",
    "print(\"Covariances shape:\", gmm.covariances_.shape)\n",
    "print(\"Weights:\", gmm.weights_)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "同学们可以自行尝试修改 n_clusters 不同值查看结果"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# 结果可视化\n",
    "# 定义绘制面部图像的函数\n",
    "def plot_faces(images, titles, h, w, n_row=2, n_col=4):\n",
    "    plt.figure(figsize=(1.8 * n_col, 2.4 * n_row))\n",
    "    plt.subplots_adjust(bottom=0, left=.01, right=.99, top=.90, hspace=.35)\n",
    "    for i in range(len(images)):\n",
    "        plt.subplot(n_row, n_col, i + 1)\n",
    "        plt.imshow(images[i].reshape((h, w)), cmap=plt.cm.gray)\n",
    "        plt.title(titles[i], size=12)\n",
    "        plt.xticks(())\n",
    "        plt.yticks(())\n",
    "\n",
    "# 获取每个聚类的中心（均值）\n",
    "mean_faces_pca = gmm.means_\n",
    "\n",
    "# 将PCA空间中的均值逆变换回原始空间\n",
    "mean_faces_original = pca.inverse_transform(mean_faces_pca)\n",
    "\n",
    "# 为每个聚类中心设置标题\n",
    "titles = [f'Cluster {i + 1}' for i in range(n_clusters)]\n",
    "\n",
    "# 绘制聚类中心面部图像\n",
    "plot_faces(mean_faces_original, titles, h, w, n_row=(n_clusters//4)+1, n_col=4)\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 自己实现高斯混合模型对电子商务数据进行聚类"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 构建高斯混合聚类学习器"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from scipy.stats import multivariate_normal\n",
    "class GaussianMixture:\n",
    "    def __init__(self,k_num=3,iter_times=100):\n",
    "        self.k = k_num\n",
    "        self.alpha = None # (k,) 混合系数\n",
    "        self.miu = None # (k,dim) 均值 \n",
    "        self.sigma = None #(k,dim,dim) 协方差矩阵\n",
    "        self.gamma = None #(n,k) 样本属于k个高斯分布的概率\n",
    "\n",
    "        self.dim = 0\n",
    "        self.n = 0\n",
    "\n",
    "        self.maxiter = iter_times\n",
    "    \n",
    "    # def gaussian_function(self,X,miu,sigma):\n",
    "    #     return np.exp(-0.5 * np.sum((X-miu) @ np.linalg.inv(sigma) * (X-miu), axis=1)) / (2*np.pi**(self.dim/2) * np.linalg.det(sigma)**0.5)\n",
    "    def gaussian_function(self,X,miu,sigma):\n",
    "        \"\"\"\n",
    "        计算多维高斯分布的概率密度函数\n",
    "        输入：\n",
    "            X: 输入数据 (n, dim)\n",
    "            miu: 均值向量 (dim,)\n",
    "            sigma: 协方差矩阵 (dim, dim)\n",
    "        输出：\n",
    "            0到1的浮点数, 高斯分布的概率密度值\n",
    "        \"\"\"\n",
    "        return multivariate_normal.pdf(X, mean=miu, cov=sigma)\n",
    "    \n",
    "    def Estep(self,data):\n",
    "        '''\n",
    "        E步: 计算每个样本属于每个高斯分布的概率\n",
    "        输入：\n",
    "            data: (n,dim)\n",
    "        输出：\n",
    "            无返回值, 对self.gamma: (n,k)进行迭代\n",
    "        '''\n",
    "        for i in range(self.k):\n",
    "            self.gamma[:,i] = self.alpha[i] * self.gaussian_function(data,self.miu[i],self.sigma[i]) \n",
    "        self.gamma /= np.sum(self.gamma,axis=1,keepdims=True)\n",
    "\n",
    "    def Mstep(self, data):\n",
    "        '''\n",
    "        M步: 更新参数\n",
    "        输入：\n",
    "            data: (n,dim)\n",
    "        输出：\n",
    "            无返回值, 对self.miu, self.sigma, self.alpha进行迭代\n",
    "        '''\n",
    "        gamma_sum = np.sum(self.gamma, axis=0)\n",
    "        self.miu = np.dot(self.gamma.T, data) / gamma_sum[:, None]\n",
    "        self.sigma = np.zeros((self.k, self.dim, self.dim))\n",
    "        for i in range(self.k):\n",
    "            x = data - self.miu[i]\n",
    "            self.sigma[i] = np.dot(self.gamma[:, i] * x.T, x) / gamma_sum[i]\n",
    "        self.alpha = gamma_sum / self.n\n",
    "    \n",
    "    # 归一化\n",
    "    def Normalization(self,data):\n",
    "        return (data - np.min(data)) / (np.max(data) - np.min(data))\n",
    "\n",
    "    def fit(self,data):\n",
    "        data = self.Normalization(data)\n",
    "        # 初始化参数\n",
    "        self.n,self.dim = data.shape\n",
    "        self.alpha = np.ones(self.k)/self.k\n",
    "        self.miu = np.random.rand(self.k,self.dim)\n",
    "        self.sigma = np.array([np.identity(self.dim)]*self.k)\n",
    "        self.gamma = np.random.rand(self.n,self.k)\n",
    "\n",
    "        # EM迭代\n",
    "        for _ in range(self.maxiter):\n",
    "            self.Estep(data)\n",
    "            self.Mstep(data)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-05T06:56:37.491263Z",
     "start_time": "2025-01-05T06:56:37.491263Z"
    }
   },
   "source": [
    "# 数据集构建\n",
    "# 设置随机种子以确保结果可重复\n",
    "np.random.seed(42)\n",
    "\n",
    "# 生成第一个高斯分布的数据\n",
    "mean1 = [0, 0, 0, 0, 0]  # 均值向量\n",
    "cov1 = np.eye(5)  # 协方差矩阵（单位矩阵）\n",
    "data1 = np.random.multivariate_normal(mean1, cov1, 100)  # 生成100个样本\n",
    "\n",
    "# 生成第二个高斯分布的数据\n",
    "mean2 = [5, 5, 5, 5, 5]\n",
    "cov2 = np.eye(5)\n",
    "data2 = np.random.multivariate_normal(mean2, cov2, 100)\n",
    "\n",
    "# 生成第三个高斯分布的数据\n",
    "mean3 = [0, 5, 0, 5, 0]\n",
    "cov3 = np.eye(5)\n",
    "data3 = np.random.multivariate_normal(mean3, cov3, 100)\n",
    "\n",
    "# 生成第四个高斯分布的数据\n",
    "mean4 = [5, 0, 5, 0, 5]\n",
    "cov4 = np.eye(5)\n",
    "data4 = np.random.multivariate_normal(mean4, cov4, 100)\n",
    "\n",
    "# 生成第五个高斯分布的数据\n",
    "mean5 = [2.5, 2.5, 2.5, 2.5, 2.5]\n",
    "cov5 = np.eye(5)\n",
    "data5 = np.random.multivariate_normal(mean5, cov5, 100)\n",
    "\n",
    "# 合并所有生成的数据\n",
    "data = np.vstack((data1, data2, data3, data4, data5))\n",
    "np.random.shuffle(data)  # 打乱数据顺序"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-05T06:56:37.492441Z",
     "start_time": "2025-01-05T06:56:37.492441Z"
    }
   },
   "source": [
    "# 标准化\n",
    "scaler = StandardScaler()\n",
    "data_normalized = scaler.fit_transform(data)  # 标准化数据，使其均值为0，标准差为1"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-05T06:56:37.493395Z",
     "start_time": "2025-01-05T06:56:37.493395Z"
    }
   },
   "source": [
    "# 降维\n",
    "pca = PCA(n_components=2)  # 初始化PCA，降到2维\n",
    "data_2d = pca.fit_transform(data_normalized)  # 对标准化后的数据进行PCA降维"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "gmm = GaussianMixture(k_num=5)\n",
    "gmm.fit(data_normalized)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-05T06:56:37.494905Z",
     "start_time": "2025-01-05T06:56:37.494905Z"
    }
   },
   "source": [
    "# 聚类结果可视化\n",
    "labels = np.argmax(gmm.gamma,axis=1)\n",
    "gmm.centers = data_normalized[:gmm.k]\n",
    "for k in range(gmm.k):\n",
    "    plt.scatter(data_normalized[labels==k][:,0],data_normalized[labels==k][:,1])\n",
    "    if(labels[labels == k].size > 0):\n",
    "        gmm.centers[k] = np.mean(data_normalized[labels==k], axis=0)\n",
    "plt.scatter(gmm.centers[:,0],gmm.centers[:,1],c='black',marker='*',s=100)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# 另一种可视化方式\n",
    "\n",
    "from matplotlib.patches import Ellipse\n",
    "fig, ax = plt.subplots()\n",
    "colors = ['red', 'blue', 'green', 'purple', 'orange']  # 确保颜色数量足够\n",
    "\n",
    "# 在学习时是根据归一化的数据进行迭代的，参数学的是归一化后的分布\n",
    "# 需要反归一化\n",
    "data_min = np.min(data_normalized, axis=0)\n",
    "data_max = np.max(data_normalized, axis=0)\n",
    "\n",
    "for i, (mean, cov, color) in enumerate(zip(gmm.miu, gmm.sigma, colors)):\n",
    "    # 反归一化\n",
    "    mean = mean * (data_max - data_min) + data_min\n",
    "    cov = cov * np.outer(data_max - data_min, data_max - data_min)\n",
    "    \n",
    "    mean_2d = pca.transform(mean.reshape(1, -1))[0]  # 将均值转换到2维空间\n",
    "    cov_2d = pca.components_ @ cov @ pca.components_.T  # 将协方差矩阵转换到2维空间\n",
    "    v, w = np.linalg.eigh(cov_2d[:2, :2])  # 计算特征值和特征向量\n",
    "    v = 2. * np.sqrt(2.) * np.sqrt(np.abs(v))  # 确保特征值为正数\n",
    "    u = w[0] / np.linalg.norm(w[0])  # 计算椭圆的旋转角度\n",
    "\n",
    "    angle = np.arctan2(u[1], u[0])  # 使用更稳健的 arctan2\n",
    "    angle = 180. * angle / np.pi  # 将角度转换为度数\n",
    "    ell = Ellipse(mean_2d, v[0], v[1], angle=angle, color=color)  # 创建椭圆对象\n",
    "    ell.set_alpha(0.5)  # 设置椭圆透明度\n",
    "    ax.add_patch(ell)  # 在图上添加椭圆\n",
    "\n",
    "ax.scatter(data_2d[:, 0], data_2d[:, 1], s=4)  # 绘制数据点"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-05T06:56:37.495918Z",
     "start_time": "2025-01-05T06:56:37.495918Z"
    }
   },
   "source": [],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "exp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
